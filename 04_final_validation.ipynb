{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea801208-e8e3-47d4-b099-ac2e952e4b4a",
   "metadata": {
    "id": "ef75d1d5"
   },
   "source": [
    "## HiFiGan Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321fdfb6-0ce1-46d9-a48f-77f071e2feb3",
   "metadata": {
    "id": "LggELooctXCT",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 17:17:53 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-12-08 17:17:53 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-12-08 17:17:53 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e0db1a-0f9a-476f-926a-0f2307de2f65",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.models import FastPitchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c8066-a8eb-48c3-bd0d-dbda23094bdb",
   "metadata": {
    "id": "lhhg2wBNtW0r"
   },
   "source": [
    "Let's also download the pretrained checkpoint that we want to finetune from. NeMo will save checkpoints to `~/.cache`, so let's move that to our current directory. \n",
    "\n",
    "*Note: please, check that `home_path` refers to your home folder. Otherwise, change it manually.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12bff96-7ed5-4a06-8e70-3fd7e40b9eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPEAKER_ID = 9017\n",
    "\n",
    "WANDB_PROJECT = \"tts-workshop\"\n",
    "WANDB_ENTITY = \"capecape\" # replace with your wandb username or team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bba2c1-f1f8-44c1-9cda-575d5770addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which split we are using\n",
    "validation_split_artifact = f'{WANDB_ENTITY}/{WANDB_PROJECT}/9017_5_split:v0'\n",
    "\n",
    "# which model\n",
    "fastpitch_artifact = f'{WANDB_ENTITY}/{WANDB_PROJECT}/model-2022-12-07_17-56-27:v2'\n",
    "hifigan_artifact   = f'{WANDB_ENTITY}/{WANDB_PROJECT}/model-32pmu5t7:v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85185ec-5fec-4b5f-81e8-9c928f9ec2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/nvidia-workshop/wandb/run-20221208_171754-3mgh1h0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/tts-workshop/runs/3mgh1h0j\" target=\"_blank\">avid-tree-21</a></strong> to <a href=\"https://wandb.ai/capecape/tts-workshop\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/capecape/tts-workshop/runs/3mgh1h0j?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f26d7c62220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"final_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edfe34-8831-440b-a07c-118ba5d73bd1",
   "metadata": {},
   "source": [
    "We can grab the fine tuned models from the `wandb` artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1917ccc-d60b-49f8-a713-adc14d1afa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-2022-12-07_17-56-27:v2, 524.07MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-32pmu5t7:v3, 969.73MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.0\n"
     ]
    }
   ],
   "source": [
    "split_artifact = wandb.use_artifact(validation_split_artifact)\n",
    "split_artifact_dir = split_artifact.download()\n",
    "\n",
    "fastpitch_artifact = wandb.use_artifact(fastpitch_artifact, type='model')\n",
    "fastpitch_artifact_dir = fastpitch_artifact.download()\n",
    "\n",
    "hifigan_artifact = wandb.use_artifact(hifigan_artifact, type='model')\n",
    "hifigan_artifact_dir = hifigan_artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed77394-98e8-40bc-b4db-30731fd2e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(path): return list(Path(path).iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad471d23-d8d9-40ed-bea8-e736e9fa7eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('artifacts/9017_5_split:v0/9017_manifest_valid_local.json'),\n",
       " PosixPath('artifacts/9017_5_split:v0/9017_manifest_train_local.json')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(split_artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebf9cf0-8b21-4ad0-8322-868551a0c34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('artifacts/model-2022-12-07_17-56-27:v2/model.ckpt')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(fastpitch_artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f42eeb-24f0-40f5-a3c6-a7739d63e3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('artifacts/model-32pmu5t7:v3/model.ckpt')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(hifigan_artifact_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b29112-ba0a-487c-9eb3-958b71776d4b",
   "metadata": {},
   "source": [
    "load FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90fdcae2-5b23-470e-8af4-a04dd137a573",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model-2022-12-07_17-56-27:v2/model.ckpt\n",
      "[NeMo I 2022-12-08 17:18:03 tokenize_and_classify:87] Creating ClassifyFst grammars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 17:18:34 experimental:27] Module <class 'nemo_text_processing.g2p.modules.IPAG2P'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-12-08 17:18:35 modules:95] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2022-12-08 17:18:35 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: 9017_manifest_train_local.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: ./fastpitch_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 30\n",
      "      pitch_fmax: 512\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 152.3\n",
      "      pitch_std: 64.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 16\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2022-12-08 17:18:35 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: 9017_manifest_valid_local.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: ./fastpitch_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 30\n",
      "      pitch_fmax: 512\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 152.3\n",
      "      pitch_std: 64.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 17:18:35 features:267] PADDING: 1\n"
     ]
    }
   ],
   "source": [
    "last_ckpt = str(ls(fastpitch_artifact_dir)[0])\n",
    "print(last_ckpt)\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)\n",
    "spec_model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce19f0d-f3cb-4a4f-bd7c-d167b3056779",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model-32pmu5t7:v3/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-12-08 17:18:38 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: hifigan_train_ft.json\n",
      "      sample_rate: 22050\n",
      "      n_segments: 8192\n",
      "      max_duration: null\n",
      "      min_duration: 0.75\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 256\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2022-12-08 17:18:38 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: hifigan_valid_ft.json\n",
      "      sample_rate: 22050\n",
      "      n_segments: 66048\n",
      "      max_duration: null\n",
      "      min_duration: 3\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 256\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-12-08 17:18:38 features:267] PADDING: 0\n",
      "[NeMo I 2022-12-08 17:18:38 features:275] STFT using exact pad\n",
      "[NeMo I 2022-12-08 17:18:38 features:267] PADDING: 0\n",
      "[NeMo I 2022-12-08 17:18:38 features:275] STFT using exact pad\n"
     ]
    }
   ],
   "source": [
    "last_ckpt = str(ls(hifigan_artifact_dir)[0])\n",
    "print(last_ckpt)\n",
    "\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(last_ckpt)\n",
    "vocoder_model.eval().cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec083108-7bc4-48c2-8896-7ad58be68926",
   "metadata": {},
   "source": [
    "Let's log the model predictions to `W&B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b3158e-2897-4f11-a7b1-765e4e88b04d",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be1a2ba-ddad-4df0-a510-76fa6b23d5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>text</th>\n",
       "      <th>duration</th>\n",
       "      <th>text_no_preprocessing</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio/dartagnan03part3_55_dumas_0101.wav</td>\n",
       "      <td>it is a pride natural to my race to pretend to...</td>\n",
       "      <td>6.28</td>\n",
       "      <td>it is a pride natural to my race to pretend to...</td>\n",
       "      <td>it is a pride natural to my race to pretend to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio/antoinetteromances4_25_dumas_0150.wav</td>\n",
       "      <td>ah if you had stayed in paris this would not h...</td>\n",
       "      <td>4.74</td>\n",
       "      <td>Ah, if you had stayed in Paris this would not ...</td>\n",
       "      <td>Ah, if you had stayed in Paris this would not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                audio_filepath  \\\n",
       "0     audio/dartagnan03part3_55_dumas_0101.wav   \n",
       "1  audio/antoinetteromances4_25_dumas_0150.wav   \n",
       "\n",
       "                                                text  duration  \\\n",
       "0  it is a pride natural to my race to pretend to...      6.28   \n",
       "1  ah if you had stayed in paris this would not h...      4.74   \n",
       "\n",
       "                               text_no_preprocessing  \\\n",
       "0  it is a pride natural to my race to pretend to...   \n",
       "1  Ah, if you had stayed in Paris this would not ...   \n",
       "\n",
       "                                     text_normalized  \n",
       "0  it is a pride natural to my race to pretend to...  \n",
       "1  Ah, if you had stayed in Paris this would not ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.read_json(Path(split_artifact_dir)/f\"{SPEAKER_ID}_manifest_valid_local.json\", lines=True)\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19d5187a-a192-4a06-a3d5-3e34cc6fd8f1",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3851c08c-e72d-49af-b51e-56902165a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(text, speaker_id):\n",
    "    \"Generate MEL and Synth Audio\"\n",
    "    spec, audio = infer(spec_model, vocoder_model, text, speaker=speaker_id)\n",
    "    return spec, audio.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba6810c3-2d73-485e-9730-52ee64867bdb",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [],
   "source": [
    "new_speaker_id = SPEAKER_ID\n",
    "duration_mins = 5\n",
    "mixing = False\n",
    "original_speaker_id = \"ljspeech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4306977-f04d-46da-92ea-a7201eb4d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=['Text', 'Real validation audio', f'HiFiGAN {new_speaker_id}', 'Spec'])\n",
    "\n",
    "sample_rate=22050\n",
    "\n",
    "for _, val_record in valid_df.iterrows():\n",
    "    speaker_spec, speaker_audio = generate_audio(val_record['text'], speaker_id=new_speaker_id)\n",
    "    row = [val_record[\"text_no_preprocessing\"],\n",
    "           wandb.Audio(val_record['audio_filepath'], sample_rate=sample_rate), \n",
    "           wandb.Audio(speaker_audio.flatten(), sample_rate=sample_rate),\n",
    "           wandb.Image(speaker_spec)]\n",
    "    table.add_data(*row)\n",
    "\n",
    "wandb.log({\"hifigan_predictions\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa2bdd02-66e8-43c7-9343-012f7e157d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-tree-21</strong>: <a href=\"https://wandb.ai/capecape/tts-workshop/runs/3mgh1h0j\" target=\"_blank\">https://wandb.ai/capecape/tts-workshop/runs/3mgh1h0j</a><br/>Synced 6 W&B file(s), 1 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_171754-3mgh1h0j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
